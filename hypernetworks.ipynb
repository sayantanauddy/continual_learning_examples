{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "developed-account",
   "metadata": {},
   "source": [
    "# Continual Learning with Hypernetworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stunning-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "parallel-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "SEED = 100\n",
    "BATCH_SIZE = 32\n",
    "NUM_TASKS = 3\n",
    "DATALEN_TRAIN = 100\n",
    "DATALEN_VAL = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interracial-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cuda():\n",
    "    \"\"\"\n",
    "    Checks if GPU is available.\n",
    "    \"\"\"    \n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    device = torch.device('cuda' if cuda_available else 'cpu')\n",
    "    return cuda_available, device\n",
    "\n",
    "def set_seed(seed=1000):\n",
    "    \"\"\"\n",
    "    Sets the seed for reproducability\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    cuda_available, _ = check_cuda()\n",
    "    if cuda_available:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sound-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyRegressionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for the toy regression task\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, task_num, datalen=100, epsilon=0.05):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            task_num (int): ID of the task to generate.\n",
    "            datalen (int): Number of data points in each task.\n",
    "            epsilon (float): Noise for target generation\n",
    "        \"\"\"\n",
    "        # Only 3 tasks can be defined\n",
    "        assert task_num in [0,1,2]\n",
    "\n",
    "        # Functions for the 3 tasks\n",
    "        true_outputs = [lambda x: x+3.0, lambda x: 2.0*(x**2.0) -1, lambda x: (x-3.0)**3]\n",
    "\n",
    "        # Input domains for the 3 tasks\n",
    "        domains = [[-4.0, -2.0], [-1.0, 1.0], [2.0, 4.0]]\n",
    "\n",
    "        self.x = torch.Tensor(datalen, ).uniform_(domains[task_num][0], domains[task_num][1])\n",
    "        self.y_true = true_outputs[task_num](self.x)\n",
    "        self.y = self.y_true + torch.normal(mean=0.0, std=epsilon, size=(datalen,))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        sample = {'x': self.x[idx], 'y_true': self.y_true[idx], 'y': self.y[idx]}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "northern-velvet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(num_tasks, datalen_train, datalen_val, batch_size, seed):\n",
    "    \"\"\"\n",
    "    Creates training and validation dataloaders for the different tasks\n",
    "    \n",
    "    Args:\n",
    "        num_tasks (int): Total number of tasks\n",
    "        datalen_train (int): Length of each training dataset\n",
    "        datalen_val (int): Length of each validation dataset\n",
    "        batch_size (int):  Length of batch size for training\n",
    "        seed (int): Random seed for reproducability\n",
    "    \"\"\"\n",
    "\n",
    "    dataloaders = list()\n",
    "\n",
    "    for task_id in range(num_tasks):\n",
    "\n",
    "        dataset = ToyRegressionDataset(task_num=task_id, datalen=(datalen_train+datalen_val))\n",
    "        train_data, val_data = random_split(dataset,\n",
    "                                            [datalen_train, datalen_val], \n",
    "                                            generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_data, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "        dataloaders.append({'train': train_loader, 'val': val_loader})\n",
    "\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "parental-triumph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloaders = create_dataloaders(num_tasks=NUM_TASKS,\n",
    "                                 datalen_train=DATALEN_TRAIN,\n",
    "                                 datalen_val=DATALEN_VAL,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 seed=SEED)\n",
    "\n",
    "colors = ['#1B9E77', '#D35C02', '#7570B3']\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,3))\n",
    "\n",
    "for task_id in range(NUM_TASKS):\n",
    "    \n",
    "    train_loader = dataloaders[task_id]['train']\n",
    "    val_loader = dataloaders[task_id]['val']\n",
    "    \n",
    "    for ax_id, data_loader in enumerate([train_loader, val_loader]):\n",
    "        x, y, y_true = list(), list(), list()\n",
    "\n",
    "        for sample in data_loader:\n",
    "            x.extend(list(sample['x'].view(-1).numpy()))\n",
    "            y_true.extend(list(sample['y_true'].view(-1).numpy()))\n",
    "            y.extend(list(sample['y'].view(-1).numpy()))\n",
    "\n",
    "        x = np.array(x)\n",
    "        y_true = np.array(y_true)\n",
    "        y = np.array(y)\n",
    "\n",
    "        sort_idx = np.argsort(x)\n",
    "        plot_x = x[sort_idx]\n",
    "        plot_y_true =  y_true[sort_idx]\n",
    "        plot_y = y[sort_idx]\n",
    "        ax[ax_id].scatter(plot_x, plot_y, color=colors[task_id])\n",
    "        ax[ax_id].plot(plot_x, plot_y_true, color=colors[task_id])\n",
    "\n",
    "_ = ax[0].set_title('Training data')\n",
    "_ = ax[1].set_title('Validation data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "reasonable-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(weights, bias):\n",
    "    # Initialize weights\n",
    "    nn.init.kaiming_uniform_(weights, a=math.sqrt(5))\n",
    "\n",
    "    # Initialize bias\n",
    "    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(weights)\n",
    "    bound = 1 / math.sqrt(fan_in)\n",
    "    nn.init.uniform_(bias, -bound, bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pretty-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_delta_theta(optimizer, use_sgd_change, lr=None, detach_dt=True):\n",
    "    \"\"\"\n",
    "    The original function can be found at https://git.io/JEHbH\n",
    "    \n",
    "    Calculate :math:`\\Delta\\theta`, i.e., the change in trainable parameters\n",
    "    (:math:`\\theta`) in order to minimize the task-specific loss.\n",
    "    \n",
    "    **Note**, one has to call :func:`torch.autograd.backward` on a\n",
    "    desired loss before calling this function, otherwise there are no gradients\n",
    "    to compute the weight change that the optimizer would cause. \n",
    "    \n",
    "    Hence, this method is called in between :func:`torch.autograd.backward` and\n",
    "    :meth:`torch.optim.Optimizer.step`.\n",
    "    \n",
    "    Note, by default, gradients are detached from the computational graph.\n",
    "    Args:\n",
    "        optimizer: The optimizer that will be used to change :math:`\\theta`.\n",
    "        use_sgd_change: If :code:`True`, then we won't calculate the actual step\n",
    "            done by the current optimizer, but the one that would be done by a\n",
    "            simple SGD optimizer.\n",
    "        lr: Has to be specified if `use_sgd_change` is :code:`True`. The\n",
    "            learning rate of the optimizer.\n",
    "        detach_dt: Whether :math:`\\Delta\\theta` should be detached from the\n",
    "            computational graph. Note, in order to backprop through\n",
    "            :math:`\\Delta\\theta`, you have to call\n",
    "            :func:`torch.autograd.backward` with `create_graph` set to\n",
    "            :code:`True` before calling this method.\n",
    "    Returns:\n",
    "        :math:`\\Delta\\theta`\n",
    "    \"\"\"\n",
    "    assert(not use_sgd_change or lr is not None)\n",
    "\n",
    "    if use_sgd_change:\n",
    "        ret = []\n",
    "        for g in optimizer.param_groups:\n",
    "            for p in g['params']:\n",
    "                if detach_dt:\n",
    "                    ret.append(-lr * p.grad.detach().clone())\n",
    "                else:\n",
    "                    ret.append(-lr * p.grad.clone())\n",
    "        return ret\n",
    "    else:\n",
    "        if isinstance(optimizer, optim.Adam):\n",
    "            return adam_step(optimizer, detach_dp=detach_dt)\n",
    "        else:\n",
    "            raise NotImplementedError(f'Not implemented for optimizer {optimizer.type}')\n",
    "                                      \n",
    "            \n",
    "def adam_step(optimizer, detach_dp=True):\n",
    "    \"\"\"\n",
    "    The original function can be found at https://git.io/JEHbH\n",
    "    \n",
    "    Performs a single optimization step using the Adam optimizer. The code\n",
    "    has been copied from: https://git.io/fjYP3\n",
    "        \n",
    "    Note, this function does not change the inner state of the given\n",
    "    optimizer object.\n",
    "    \n",
    "    Note, gradients are cloned and detached by default.\n",
    "    \n",
    "    Args:\n",
    "        optimizer: An instance of class :class:`torch.optim.Adam`.\n",
    "        detach_dp: Whether gradients are detached from the computational\n",
    "            graph. Note, :code:`False` only makes sense if\n",
    "            func:`torch.autograd.backward` was called with the argument\n",
    "            `create_graph` set to :code:`True`.\n",
    "    Returns:\n",
    "        A list of gradient changes `d_p` that would be applied by this\n",
    "        optimizer to all parameters when calling :meth:`torch.optim.Adam.step`.\n",
    "    \"\"\"\n",
    "    assert (isinstance(optimizer, optim.Adam))\n",
    "\n",
    "    d_ps = []\n",
    "\n",
    "    for group in optimizer.param_groups:\n",
    "        for p in group['params']:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "\n",
    "            if detach_dp:\n",
    "                grad = p.grad.detach().clone()\n",
    "            else:\n",
    "                grad = p.grad.clone()\n",
    "\n",
    "            if grad.is_sparse:\n",
    "                raise RuntimeError(\n",
    "                    'Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "            amsgrad = group['amsgrad']\n",
    "            if amsgrad and not detach_dp:\n",
    "                raise ValueError('Cannot backprop through optimizer step if ' +\n",
    "                                 '\"amsgrad\" is enabled.')\n",
    "\n",
    "            orig_state = dict(optimizer.state[p])\n",
    "            state = dict()\n",
    "\n",
    "            # State initialization\n",
    "            if len(orig_state) == 0:\n",
    "                orig_state['step'] = 0\n",
    "                # Exponential moving average of gradient values\n",
    "                orig_state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                # Exponential moving average of squared gradient values\n",
    "                orig_state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                if amsgrad:\n",
    "                    # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                    orig_state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "            # Copy original state.\n",
    "            state['step'] = int(orig_state['step'])\n",
    "            state['exp_avg'] = orig_state['exp_avg'].clone()\n",
    "            state['exp_avg_sq'] = orig_state['exp_avg_sq'].clone()\n",
    "            if amsgrad:\n",
    "                state['max_exp_avg_sq'] = orig_state['max_exp_avg_sq'].clone()\n",
    "\n",
    "            exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "            if amsgrad:\n",
    "                max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "            beta1, beta2 = group['betas']\n",
    "\n",
    "            state['step'] += 1\n",
    "\n",
    "            if group['weight_decay'] != 0:\n",
    "                #grad.add_(group['weight_decay'], p.data)\n",
    "                grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "            # Decay the first and second moment running average coefficient\n",
    "            \n",
    "            #exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "            \n",
    "            # The above line throws the following warning in torch 1.8:\n",
    "            # ..\\torch\\csrc\\utils\\python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
    "            # add_(Number alpha, Tensor other)\n",
    "            # Consider using one of the following signatures instead:\n",
    "            # add_(Tensor other, *, Number alpha)\n",
    "            # Changing the add_ function according to the suggestion at https://git.io/JEpUi\n",
    "            exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "            #exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "            # The above line also throws a warning. See the link above\n",
    "            exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "            if amsgrad:\n",
    "                # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                # Use the max. for normalizing running avg. of gradient\n",
    "                denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "            else:\n",
    "                #denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                denom = exp_avg_sq.sqrt() + group['eps']\n",
    "\n",
    "            bias_correction1 = 1 - beta1 ** state['step']\n",
    "            bias_correction2 = 1 - beta2 ** state['step']\n",
    "            step_size = group['lr'] * math.sqrt(\n",
    "                bias_correction2) / bias_correction1\n",
    "\n",
    "            d_ps.append(-step_size * (exp_avg / denom))\n",
    "\n",
    "    return d_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "distributed-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fix_target_reg(hnet, \n",
    "                        task_id, \n",
    "                        targets=None, \n",
    "                        dTheta=None, \n",
    "                        prev_theta=None,\n",
    "                        prev_task_embs=None, \n",
    "                        reg_scaling=None):\n",
    "    \"\"\"\n",
    "    The original function can be found at https://git.io/JEHxq\n",
    "    \n",
    "    This function computes the regularization loss (the second part of equation 2)\n",
    "    \n",
    "    This regularizer simply restricts the output-mapping for previous\n",
    "    task embeddings. I.e., for all :math:`j < \\text{task\\_id}` minimize:\n",
    "    .. math::\n",
    "        \\lVert \\text{target}_j - h(c_j, \\theta + \\Delta\\theta) \\rVert^2\n",
    "    where :math:`c_j` is the current task embedding for task :math:`j` (and we\n",
    "    assumed that `dTheta` was passed).\n",
    "    \n",
    "    Args:\n",
    "        hnet: The hypernetwork whose output should be regularized. Has to\n",
    "            implement the interface CLHyperNetInterface.\n",
    "        task_id: The ID of the current task (the one that is used to\n",
    "            compute dTheta.\n",
    "        targets: A list of outputs of the hypernetwork. Each list entry must\n",
    "            have the output shape as returned by the forward method of this\n",
    "            class. Note, this method doesn't detach targets. If desired,\n",
    "            that should be done before calling this method.\n",
    "        dTheta (optional): The current direction of weight change for the\n",
    "            internal weights of the hypernetwork evaluated on the task-specific\n",
    "            loss, i.e., the weight change that would be applied to theta. This\n",
    "            regularizer aims to modify this direction, such that the hypernet\n",
    "            output for embeddings of previous tasks remains unaffected.\n",
    "            Note, this function does not detach dTheta. It is up to the\n",
    "            user to decide whether dTheta should be a constant vector or\n",
    "            might depend on parameters of the hypernet.\n",
    "        prev_theta (optional): If given, `prev_task_embs` but not `targets`\n",
    "            has to be specified. `prev_theta` is expected to be the internal\n",
    "            weights theta prior to learning the current task. Hence, it can be\n",
    "            used to compute the targets on the fly (which is more memory\n",
    "            efficient (constant memory), but more computationally demanding).\n",
    "            The computed targets will be detached from the computational graph.\n",
    "            Independent of the current hypernet mode, the targets are computed\n",
    "            in \"eval\" mode.\n",
    "        prev_task_embs (optional): If given, `prev_theta` but not `targets`\n",
    "            has to be specified. \"prev_task_embs\" are the task embeddings \n",
    "            learned prior to learning the current task. It is sufficient to\n",
    "            only pass the task embeddings for tasks with ID smaller than the\n",
    "            current one (only those tasks that are regularized).\n",
    "            See docstring of \"prev_theta\" for more details.\n",
    "        reg_scaling (optional): If specified, the regulariation terms for the \n",
    "            different tasks are scaled arcording to the entries of this list.\n",
    "    Returns:\n",
    "        The value of the regularizer.\n",
    "    \"\"\"\n",
    "    assert(task_id > 0)\n",
    "    assert(targets is None or len(targets) == task_id)\n",
    "    assert(targets is None or (prev_theta is None and prev_task_embs is None))\n",
    "    assert(prev_theta is None or prev_task_embs is not None)\n",
    "    assert(prev_task_embs is None or len(prev_task_embs) >= task_id)\n",
    "    assert(reg_scaling is None or len(reg_scaling) >= task_id)\n",
    "\n",
    "    # Number of tasks to be regularized.\n",
    "    num_regs = task_id\n",
    "    \n",
    "    # Regularization is done for previous task_ids (task_ids 0, 1, ..., task_id-1)\n",
    "    ids_to_reg = list(range(num_regs))\n",
    "\n",
    "    # Initialize the regularization value\n",
    "    reg = 0\n",
    "\n",
    "    for i in ids_to_reg:\n",
    "        \n",
    "        # Generate params of the target network for task i with \n",
    "        # an increment of the hnet parameters (dTheta)\n",
    "        weights_predicted = hnet.forward(task_id=i, dTheta=dTheta)\n",
    "\n",
    "        if targets is not None:\n",
    "            target = targets[i]\n",
    "        else:\n",
    "            # Compute targets in eval mode!\n",
    "            hnet_mode = hnet.training\n",
    "            hnet.eval()\n",
    "\n",
    "            # Compute target on the fly using previous hnet.\n",
    "            with torch.no_grad():\n",
    "                target = hnet.forward(theta=prev_theta,\n",
    "                                      task_emb=prev_task_embs[i])\n",
    "                \n",
    "            target = [d.detach().clone() for d in target]\n",
    "\n",
    "            hnet.train(mode=hnet_mode)\n",
    "\n",
    "        # Regularize all weights of the target network.\n",
    "        # Removing the batch dimension from the generated parameters\n",
    "        W_target = torch.cat([w.view(-1) for w in target])\n",
    "        W_predicted = torch.cat([w.view(-1) for w in weights_predicted])\n",
    "\n",
    "        # Compute the regularization for task i\n",
    "        reg_i = (W_target - W_predicted).pow(2).sum()\n",
    "\n",
    "        if reg_scaling is not None:\n",
    "            reg += reg_scaling[i] * reg_i\n",
    "        else:\n",
    "            reg += reg_i\n",
    "\n",
    "    return reg / num_regs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "international-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_targets(task_id, hnet):\n",
    "    \"\"\"\n",
    "    For all :math:`j < \\text{task\\_id}`, compute the output of the\n",
    "    hypernetwork. This output will be detached from the graph and cloned before\n",
    "    being added to the return list of this function.\n",
    "    \n",
    "    Note, if these targets don't change during training, it would be more memory\n",
    "    efficient to store the weights :math:`\\theta^*` of the hypernetwork (which\n",
    "    is a fixed amount of memory compared to the variable number of tasks).\n",
    "    Though, it is more computationally expensive to recompute\n",
    "    :math:`h(c_j, \\theta^*)` for all :math:`j < \\text{task\\_id}` everytime the\n",
    "    target is needed.\n",
    "    \n",
    "    Note, this function sets the hypernet temporarily in eval mode. No gradients\n",
    "    are computed.\n",
    "    \n",
    "    Args:\n",
    "        task_id: The ID of the current task.\n",
    "        hnet: An instance of the hypernetwork before learning a new task\n",
    "            (i.e., the hypernetwork has the weights :math:`\\theta^*` necessary\n",
    "            to compute the targets).\n",
    "    Returns:\n",
    "        An empty list, if `task_id` is 0. Otherwise, a list of `task_id`-1\n",
    "        targets. These targets can be passed to the method\n",
    "        :func:`calc_fix_target_reg` while training on the new task.\n",
    "    \"\"\"\n",
    "    # We temporarily switch to eval mode for target computation (e.g., to get\n",
    "    # rid of training stochasticities such as dropout).\n",
    "    hnet_mode = hnet.training\n",
    "    hnet.eval()\n",
    "\n",
    "    ret = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j in range(task_id):\n",
    "            W = hnet.forward(task_id=j)\n",
    "            ret.append([d.detach().clone() for d in W])\n",
    "\n",
    "    hnet.train(mode=hnet_mode)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "forbidden-official",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 target_shapes, \n",
    "                 layers=[50, 100], \n",
    "                 te_dim=8, \n",
    "                 activation_fn=torch.nn.ReLU(),\n",
    "                 ce_dim=None, \n",
    "                 dropout_rate=-1):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            target_shapes: A list of list of integers, denoting the shape of each \n",
    "                           parameter tensor in the target network (hence, determining\n",
    "                           the output of this network).\n",
    "            layers: A list of integers, each indicating the size of a hidden layer \n",
    "                    in this network.\n",
    "            te_dim: The dimensionality of the task embeddings.\n",
    "            activation_fn: The nonlinearity used in hidden layers.\n",
    "            use_bias: Whether layers may have bias terms.\n",
    "            ce_dim (optional): The dimensionality of any additional embeddings,\n",
    "                (in addition to the task embedding) that will be used as input\n",
    "                to the hypernetwork. If this option is ``None``, no additional\n",
    "                input is expected. Otherwise, an additional embedding has to be\n",
    "                passed to the :meth:`forward` method (see argument\n",
    "                ``ext_inputs``). A typical usecase would be a chunk embedding.\n",
    "            dropout_rate (optional): If -1, no dropout will be applied.\n",
    "                Otherwise a number between 0 and 1 is expected, denoting the\n",
    "                dropout of hidden layers.           \n",
    "        \"\"\"\n",
    "        nn.Module.__init__(self)\n",
    "        \n",
    "        assert(len(target_shapes) > 0)\n",
    "        \n",
    "        # Set the class members\n",
    "        self.target_shapes = target_shapes\n",
    "        self.layers = layers\n",
    "        self.te_dim = te_dim\n",
    "        self.activation_fn = activation_fn\n",
    "        self.ce_dim = ce_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Empty parameter list of task embeddings\n",
    "        # Before learning each task, an embedding is\n",
    "        # to be created for that task\n",
    "        self.task_embs = nn.ParameterList()\n",
    "        \n",
    "        # Create layers of the hypernetwork\n",
    "        self.gen_layers()\n",
    "        \n",
    "        # Create a dropout layer to be used in the forward training pass\n",
    "        self.dropout = None\n",
    "        if dropout_rate != -1:\n",
    "            assert(dropout_rate >= 0.0 and dropout_rate <= 1.0)\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "            \n",
    "        # List of shapes of learnable parameters of the hypernet\n",
    "        self.theta_shapes = self.hidden_dims + self.out_dims\n",
    "                        \n",
    "    def gen_new_task_emb(self):\n",
    "        \"\"\"\n",
    "        Creates a new task embedding before learning a task\n",
    "        \"\"\"\n",
    "        self.task_embs.append(nn.Parameter(data=torch.Tensor(self.te_dim), requires_grad=True))\n",
    "        torch.nn.init.normal_(self.task_embs[-1], mean=0., std=1.)\n",
    "        \n",
    "    def get_task_embs(self):\n",
    "        \"\"\"Return a list of all task embeddings.\n",
    "        Returns:\n",
    "            A list of Parameter tensors.\n",
    "        \"\"\"\n",
    "        return self.task_embs\n",
    "\n",
    "    def get_task_emb(self, task_id):\n",
    "        \"\"\"Return the task embedding corresponding to a given task id.\n",
    "        Args:\n",
    "            task_id: Determines the task for which the embedding should be\n",
    "                returned.\n",
    "        Returns:\n",
    "            A list of Parameter tensors.\n",
    "        \"\"\"\n",
    "        return self.task_embs[task_id]\n",
    "        \n",
    "    def gen_layers(self):\n",
    "        \n",
    "        self.hidden_dims = []\n",
    "        prev_dim = self.te_dim\n",
    "        \n",
    "        # If chunk embeddings are used as inputs\n",
    "        # in addition to the task embeddings\n",
    "        if self.ce_dim is not None:\n",
    "            prev_dim += self.ce_dim\n",
    "            \n",
    "        # Shapes of hidden layers\n",
    "        # Creating a list of lists with the size of the\n",
    "        # weight matrices and bias vectors\n",
    "        # [[w0.shape[0], w0.shape[1]], [b0.shape[0]], ...., ]\n",
    "        for i, size in enumerate(self.layers):\n",
    "            # Append a weight matrix\n",
    "            self.hidden_dims.append([size, prev_dim])\n",
    "            # Append a bias vector\n",
    "            self.hidden_dims.append([size])\n",
    "            prev_dim = size\n",
    "        self.last_hidden_size = prev_dim\n",
    "        \n",
    "        # Shapes of output layers\n",
    "        # Each layer of the target network corresponds to \n",
    "        # a separate output head of the hypernetwork\n",
    "        self.out_dims = []\n",
    "        for i, dims in enumerate(self.target_shapes):\n",
    "            nouts = np.prod(dims)\n",
    "            # Append a weight matrix\n",
    "            self.out_dims.append([nouts, self.last_hidden_size])\n",
    "            # Append a bias vector\n",
    "            self.out_dims.append([nouts])\n",
    "                \n",
    "        # Creating the trainable parameters\n",
    "        self.theta = nn.ParameterList()\n",
    "        all_dims = self.hidden_dims + self.out_dims\n",
    "        for i, dims in enumerate(all_dims):\n",
    "            self.theta.append(nn.Parameter(torch.Tensor(*dims), requires_grad=True))\n",
    "\n",
    "        # Initialize the trainable parameters\n",
    "        # Odd elements of self.theta are weihghts\n",
    "        # Even members are biases\n",
    "        for i in range(0, len(self.theta), 2):\n",
    "            init_params(self.theta[i], self.theta[i + 1])\n",
    "    \n",
    "    def forward(self, task_id, dTheta=None, squeeze=True):\n",
    "    \n",
    "        # For the second step of the loss, we need to evaluate the\n",
    "        # networks performance using the increment dtheta\n",
    "        if dTheta is not None:\n",
    "            assert(len(dTheta) == len(self.theta_shapes))\n",
    "\n",
    "            weights = []\n",
    "            for i, t in enumerate(self.theta):\n",
    "                weights.append(t + dTheta[i])\n",
    "        else:\n",
    "            weights = self.theta\n",
    "            \n",
    "        # Select the appropriate task embedding\n",
    "        assert len(self.task_embs) > 0, \"List of task embeddings is empty\"\n",
    "        assert task_id < len(self.task_embs), \"Invalid task_id\"\n",
    "        task_emb = self.task_embs[task_id]\n",
    "        \n",
    "        # We pass the task embedding for the current task as the input\n",
    "        # to the hypernetwork. Since there is only 1 embedding per task\n",
    "        # we set the batch size to 1\n",
    "        batch_size = 1\n",
    "        h = task_emb.expand(batch_size, self.te_dim)\n",
    "        \n",
    "        # Forward propagate the task embedding through the hidden layers\n",
    "        # Note that the hidden layers form the trunk of the hypernetwork\n",
    "        # that is common to all the output layers\n",
    "        for i in range(0, len(self.hidden_dims), 2):\n",
    "            h = F.linear(h, weights[i], bias=weights[i+1])\n",
    "            if self.activation_fn is not None:\n",
    "                h = self.activation_fn(h)\n",
    "            if self.dropout is not None:\n",
    "                h = self.dropout(h)\n",
    "                \n",
    "        # The hypernetwork has a separate output head for each parameter matrix/vector\n",
    "        # of the target network. The output of the last hidden layer is passed\n",
    "        # through the layers corresponding each of the output heads. These\n",
    "        # outputs are stored in `outputs`\n",
    "        outputs = []\n",
    "        j = 0\n",
    "        for i in range(len(self.hidden_dims), len(self.theta_shapes), 2):\n",
    "            W = F.linear(h, weights[i], bias=weights[i+1])\n",
    "            W = W.view(batch_size, *self.target_shapes[j])\n",
    "            # Remove the batch dimension from the output which will be\n",
    "            # used to intialize the layers of the target network\n",
    "            if squeeze:\n",
    "                W = torch.squeeze(W, dim=0)\n",
    "            outputs.append(W)\n",
    "            j += 1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "copyrighted-movement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyperNetwork(\n",
      "  (activation_fn): ReLU()\n",
      "  (task_embs): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 2])\n",
      "  (theta): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 10x2]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 10]\n",
      "      (2): Parameter containing: [torch.FloatTensor of size 10x10]\n",
      "      (3): Parameter containing: [torch.FloatTensor of size 10]\n",
      "      (4): Parameter containing: [torch.FloatTensor of size 10x10]\n",
      "      (5): Parameter containing: [torch.FloatTensor of size 10]\n",
      "      (6): Parameter containing: [torch.FloatTensor of size 10x10]\n",
      "      (7): Parameter containing: [torch.FloatTensor of size 10]\n",
      "      (8): Parameter containing: [torch.FloatTensor of size 100x10]\n",
      "      (9): Parameter containing: [torch.FloatTensor of size 100]\n",
      "      (10): Parameter containing: [torch.FloatTensor of size 10x10]\n",
      "      (11): Parameter containing: [torch.FloatTensor of size 10]\n",
      "      (12): Parameter containing: [torch.FloatTensor of size 10x10]\n",
      "      (13): Parameter containing: [torch.FloatTensor of size 10]\n",
      "      (14): Parameter containing: [torch.FloatTensor of size 1x10]\n",
      "      (15): Parameter containing: [torch.FloatTensor of size 1]\n",
      "  )\n",
      ")\n",
      "torch.Size([10, 1])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "hnet = HyperNetwork(layers=[10, 10], te_dim=2, target_shapes=[[10,1],[10],[10,10],[10],[1,10],[1]])\n",
    "hnet.gen_new_task_emb()\n",
    "print(hnet)\n",
    "op = hnet(0)\n",
    "for o in op:\n",
    "    print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "frozen-occasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_in=1, \n",
    "                 n_out=1, \n",
    "                 hidden_layers=[10, 10],\n",
    "                 activation_fn=torch.nn.ReLU(), \n",
    "                 use_bias=True, \n",
    "                 no_weights=False,\n",
    "                 init_weights=None, \n",
    "                 dropout_rate=-1, \n",
    "                 use_batch_norm=False, \n",
    "                 bn_track_stats=True,\n",
    "                 distill_bn_stats=False, \n",
    "                 out_fn=None):\n",
    "\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.activation_fn = activation_fn\n",
    "        self.no_weights = no_weights\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.bn_track_stats = bn_track_stats\n",
    "        self.distill_bn_stats = distill_bn_stats and use_batch_norm\n",
    "        self.out_fn = out_fn\n",
    "\n",
    "        self.has_bias = use_bias\n",
    "        self.has_fc_out = True\n",
    "\n",
    "        # We need to make sure that the last 2 entries of `weights` correspond\n",
    "        # to the weight matrix and bias vector of the last layer.\n",
    "        self.mask_fc_out = True\n",
    "        self.has_linear_out = True if out_fn is None else False\n",
    "\n",
    "        self.weights = None if no_weights else nn.ParameterList()\n",
    "\n",
    "        if dropout_rate != -1:\n",
    "            assert(dropout_rate >= 0. and dropout_rate <= 1.)\n",
    "            self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        ### Define and initialize batch norm weights.\n",
    "        # TODO Check later\n",
    "        self.batchnorm_layers = nn.ModuleList() if use_batch_norm else None\n",
    "\n",
    "        if use_batch_norm:\n",
    "            if distill_bn_stats:\n",
    "                self.hyper_shapes_distilled = []\n",
    "\n",
    "            bn_ind = 0\n",
    "            for i, n in enumerate(hidden_layers):\n",
    "                bn_layer = BatchNormLayer(n, affine=not no_weights, track_running_stats=bn_track_stats)\n",
    "                self.batchnorm_layers.append(bn_layer)\n",
    "\n",
    "                if not no_weights:\n",
    "                    self.weights.extend(bn_layer.weights)\n",
    "\n",
    "                if distill_bn_stats:\n",
    "                    self.hyper_shapes_distilled.extend( \\\n",
    "                        [list(p.shape) for p in bn_layer.get_stats(0)])\n",
    "\n",
    "                # FIXME ugly code. Move initialization somewhere else.\n",
    "                if not no_weights and init_weights is not None:\n",
    "                    assert(len(bn_layer.weights) == 2)\n",
    "                    for ii in range(2):\n",
    "                        assert(np.all(np.equal( \\\n",
    "                                list(init_weights[bn_ind].shape),\n",
    "                                list(bn_layer.weights[ii].shape))))\n",
    "                        bn_layer.weights[ii].data = init_weights[bn_ind]\n",
    "                        bn_ind += 1\n",
    "\n",
    "            if init_weights is not None:\n",
    "                init_weights = init_weights[bn_ind:]\n",
    "\n",
    "        # Compute shapes of linear layers.\n",
    "        linear_shapes = TargetNetwork.weight_shapes(n_in=n_in, \n",
    "                                                    n_out=n_out,\n",
    "                                                    hidden_layers=hidden_layers, \n",
    "                                                    use_bias=use_bias)\n",
    "\n",
    "        self.layer_weight_tensors = nn.ParameterList()\n",
    "        self.layer_bias_vectors = nn.ParameterList()\n",
    "\n",
    "        if no_weights:\n",
    "            return\n",
    "\n",
    "        ### Define and initialize linear weights.\n",
    "        for i, dims in enumerate(linear_shapes):\n",
    "            self.weights.append(nn.Parameter(torch.Tensor(*dims),\n",
    "                                              requires_grad=True))\n",
    "            if len(dims) == 1:\n",
    "                self.layer_bias_vectors.append(self.weights[-1])\n",
    "            else:\n",
    "                self.layer_weight_tensors.append(self.weights[-1])\n",
    "\n",
    "        # If weights are populated by a hypernetwork\n",
    "        if init_weights is not None:\n",
    "            assert(len(init_weights) == len(linear_shapes))\n",
    "            for i in range(len(init_weights)):\n",
    "                assert(np.all(np.equal(list(init_weights[i].shape),\n",
    "                                       linear_shapes[i])))\n",
    "                \n",
    "                # If bias is used, param numbers 0,2,4,... are weights\n",
    "                # and param numbers 1,3,5,... are biases\n",
    "                if use_bias:\n",
    "                    if i % 2 == 0:\n",
    "                        self.layer_weight_tensors[i//2].data = init_weights[i]\n",
    "                    else:\n",
    "                        self.layer_bias_vectors[i//2].data = init_weights[i]\n",
    "                else:\n",
    "                    self.layer_weight_tensors[i].data = init_weights[i]\n",
    "        # Else parameters need to be generated and initialized\n",
    "        else:\n",
    "            for i in range(len(self.layer_weight_tensors)):\n",
    "                if use_bias:\n",
    "                    init_params(self.layer_weight_tensors[i],\n",
    "                                self.layer_bias_vectors[i])\n",
    "                else:\n",
    "                    init_params(self.layer_weight_tensors[i])\n",
    "    \n",
    "    def forward(self, \n",
    "                x, \n",
    "                weights=None, \n",
    "                distilled_params=None, \n",
    "                condition=None):\n",
    "        \"\"\"Compute the output :math:`y` of this network given the input\n",
    "        :math:`x`.\n",
    "        \n",
    "        Args:\n",
    "            weights (list): \n",
    "            distilled_params: Will be passed as ``running_mean`` and\n",
    "                ``running_var`` arguments of method\n",
    "                :meth:`utils.batchnorm_layer.BatchNormLayer.forward` if\n",
    "                batch normalization is used.\n",
    "            condition (optional, int or dict): If ``int`` is provided, then this\n",
    "                argument will be passed as argument ``stats_id`` to the method\n",
    "                :meth:`utils.batchnorm_layer.BatchNormLayer.forward` if\n",
    "                batch normalization is used.\n",
    "                If a ``dict`` is provided instead, the following keywords are\n",
    "                allowed:\n",
    "                    - ``bn_stats_id``: Will be handled as ``stats_id`` of the\n",
    "                      batchnorm layers as described above.\n",
    "                    - ``cmod_ckpt_id``: Will be passed as argument ``ckpt_id``\n",
    "                      to the method\n",
    "                      :meth:`utils.context_mod_layer.ContextModLayer.forward`.\n",
    "        Returns:\n",
    "            (tuple): Tuple containing:\n",
    "            - **y**: The output of the network.\n",
    "            - **h_y** (optional): If ``out_fn`` was specified in the\n",
    "              constructor, then this value will be returned. It is the last\n",
    "              hidden activation (before the ``out_fn`` has been applied).\n",
    "        \"\"\"\n",
    "        if (not self.no_weights) and (weights is None):\n",
    "            raise Exception('Network was generated without weights. ' +\n",
    "                            'Hence, \"weights\" option may not be None.')\n",
    "\n",
    "        #weights = self.weights\n",
    "        int_weights = weights\n",
    "        \n",
    "        bn_ind = 0\n",
    "\n",
    "        if self.use_batch_norm:\n",
    "            n_bn = 2 * len(self.batchnorm_layers)\n",
    "            bn_weights = int_weights[:n_bn]\n",
    "            layer_weights = int_weights[n_bn:]\n",
    "        else:\n",
    "            layer_weights = int_weights\n",
    "\n",
    "        w_weights = []\n",
    "        b_weights = []\n",
    "        for i, p in enumerate(layer_weights):\n",
    "            if self.has_bias and i % 2 == 1:\n",
    "                b_weights.append(p)\n",
    "            else:\n",
    "                w_weights.append(p)\n",
    "\n",
    "        ######################################\n",
    "        ### Select batchnorm running stats ###\n",
    "        ######################################\n",
    "        # TODO Omit for now\n",
    "        if self.use_batch_norm:\n",
    "            nn = len(self.batchnorm_layers)\n",
    "            running_means = [None] * nn\n",
    "            running_vars = [None] * nn\n",
    "\n",
    "        if distilled_params is not None:\n",
    "            if not self.distill_bn_stats:\n",
    "                raise ValueError('Argument \"distilled_params\" can only be ' +\n",
    "                                 'provided if the return value of ' +\n",
    "                                 'method \"distillation_targets()\" is not None.')\n",
    "            shapes = self.hyper_shapes_distilled\n",
    "            assert(len(distilled_params) == len(shapes))\n",
    "            for i, s in enumerate(shapes):\n",
    "                assert(np.all(np.equal(s, list(distilled_params[i].shape))))\n",
    "\n",
    "            # Extract batchnorm stats from distilled_params\n",
    "            for i in range(0, len(distilled_params), 2):\n",
    "                running_means[i//2] = distilled_params[i]\n",
    "                running_vars[i//2] = distilled_params[i+1]\n",
    "\n",
    "        elif self.use_batch_norm and self.bn_track_stats and \\\n",
    "                bn_cond is None:\n",
    "            for i, bn_layer in enumerate(self.batchnorm_layers):\n",
    "                running_means[i], running_vars[i] = bn_layer.get_stats()\n",
    "\n",
    "        ###########################\n",
    "        ### Forward Computation ###\n",
    "        ###########################\n",
    "        # If the input does not have a batch dimension, insert it\n",
    "        if len(list(x.shape)) == 1:\n",
    "            hidden = x.unsqueeze(-1)\n",
    "        else:\n",
    "            hidden = x\n",
    "\n",
    "        for l in range(len(w_weights)):\n",
    "            W = w_weights[l]\n",
    "            if self.has_bias:\n",
    "                b = b_weights[l]\n",
    "            else:\n",
    "                b = None\n",
    "\n",
    "            # Linear layer.\n",
    "            hidden = F.linear(hidden, W, bias=b)\n",
    "\n",
    "            # Only for hidden layers.\n",
    "            if l < len(w_weights) - 1:\n",
    "\n",
    "                # Batch norm\n",
    "                if self.use_batch_norm:\n",
    "                    hidden = self.batchnorm_layers[bn_ind].forward(hidden,\n",
    "                                                                   running_mean=running_means[bn_ind],\n",
    "                                                                   running_var=running_vars[bn_ind],\n",
    "                                                                   weight=bn_weights[2*bn_ind],\n",
    "                                                                   bias=bn_weights[2*bn_ind+1], stats_id=bn_cond)\n",
    "                    bn_ind += 1\n",
    "\n",
    "                # Dropout\n",
    "                if self.dropout_rate != -1:\n",
    "                    hidden = self.dropout(hidden)\n",
    "\n",
    "                # Non-linearity\n",
    "                if self.activation_fn is not None:\n",
    "                    hidden = self.activation_fn(hidden)\n",
    "\n",
    "        if self.out_fn is not None:\n",
    "            return self.out_fn(hidden), hidden\n",
    "\n",
    "        return hidden\n",
    "    \n",
    "    @staticmethod\n",
    "    def weight_shapes(n_in=1, n_out=1, hidden_layers=[10, 10], use_bias=True):\n",
    "        \"\"\"Compute the tensor shapes of all parameters in a fully-connected\n",
    "        network.\n",
    "        Args:\n",
    "            n_in: Number of inputs.\n",
    "            n_out: Number of output units.\n",
    "            hidden_layers: A list of ints, each number denoting the size of a\n",
    "                hidden layer.\n",
    "            use_bias: Whether the FC layers should have biases.\n",
    "        Returns:\n",
    "            A list of list of integers, denoting the shapes of the individual\n",
    "            parameter tensors.\n",
    "        \"\"\"\n",
    "        shapes = []\n",
    "\n",
    "        prev_dim = n_in\n",
    "        layer_out_sizes = hidden_layers + [n_out]\n",
    "        for i, size in enumerate(layer_out_sizes):\n",
    "            shapes.append([size, prev_dim])\n",
    "            if use_bias:\n",
    "                shapes.append([size])\n",
    "            prev_dim = size\n",
    "\n",
    "        return shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "molecular-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions from https://git.io/JExEV (utils/misc.py)\n",
    "\n",
    "def str_to_ints(str_arg):\n",
    "    \"\"\"Helper function to convert a list of comma separated strings into\n",
    "    integers.\n",
    "    Args:\n",
    "        str_arg: String containing list of comma-separated ints. For convenience\n",
    "            reasons, we allow the user to also pass single integers that a put\n",
    "            into a list of length 1 by this function.\n",
    "    Returns:\n",
    "        List of integers.\n",
    "    \"\"\"\n",
    "    if isinstance(str_arg, int):\n",
    "        return [str_arg]\n",
    "\n",
    "    if len(str_arg) > 0:\n",
    "        return [int(s) for s in str_arg.split(',')]\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "def list_to_str(list_arg, delim=' '):\n",
    "    \"\"\"Convert a list of numbers into a string.\n",
    "    Args:\n",
    "        list_arg: List of numbers.\n",
    "        delim (optional): Delimiter between numbers.\n",
    "    Returns:\n",
    "        List converted to string.\n",
    "    \"\"\"\n",
    "    ret = ''\n",
    "    for i, e in enumerate(list_arg):\n",
    "        if i > 0:\n",
    "            ret += delim\n",
    "        ret += str(e)\n",
    "    return ret\n",
    "\n",
    "def str_to_act(act_str):\n",
    "    \"\"\"Convert the name of an activation function into the actual PyTorch\n",
    "    activation function.\n",
    "    Args:\n",
    "        act_str: Name of activation function (as defined by command-line\n",
    "            arguments).\n",
    "    Returns:\n",
    "        Torch activation function instance or ``None``, if ``linear`` is given.\n",
    "    \"\"\"\n",
    "    if act_str == 'linear':\n",
    "        act = None\n",
    "    elif act_str == 'sigmoid':\n",
    "        act = torch.nn.Sigmoid()\n",
    "    elif act_str == 'relu':\n",
    "        act = torch.nn.ReLU()\n",
    "    elif act_str == 'elu':\n",
    "        act = torch.nn.ELU()\n",
    "    else:\n",
    "        raise Exception('Activation function %s unknown.' % act_str)\n",
    "    return act\n",
    "\n",
    "class Dict2Class(object):\n",
    "    \"\"\"\n",
    "    Turns a dictionary into a class\n",
    "    \"\"\"\n",
    "    def __init__(self, my_dict):\n",
    "        for key in my_dict:\n",
    "            setattr(self, key, my_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "partial-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reg(task_id, dataloader, tnet, hnet, device, config):\n",
    "    \"\"\"\n",
    "    \n",
    "    Train the network using the task-specific loss plus a regularizer that\n",
    "    should weaken catastrophic forgetting.\n",
    "    .. math::\n",
    "        \\text{loss} = \\text{task\\_loss} + \\beta * \\text{regularizer}\n",
    "    Args:\n",
    "        task_id: The index of the task on which we train.\n",
    "        dataloader: Training dataloader.\n",
    "        tnet: The model of the target network.\n",
    "        hnet: The model of the hyper network.\n",
    "        device: Torch device (cpu or gpu).\n",
    "        config: The command line arguments.\n",
    "    \"\"\"\n",
    "    print('Training network ...')\n",
    "\n",
    "    tnet.train()\n",
    "    hnet.train()\n",
    "    \n",
    "    # Create a new task embedding for this task\n",
    "    hnet.gen_new_task_emb()\n",
    "\n",
    "    # Get the parameters generated by the hnet for all tasks\n",
    "    # preceeding the current task_id. This will be used for \n",
    "    # calculating the regularized targets.\n",
    "    if config.beta > 0:\n",
    "        targets = get_current_targets(task_id, hnet)\n",
    "\n",
    "    # Trainable weights and biases of the hnet\n",
    "    regularized_params = list(hnet.theta)\n",
    "               \n",
    "    # For optimizing the weights and biases of the hnet\n",
    "    theta_optimizer = optim.Adam(regularized_params, lr=config.lr_hyper)\n",
    "    \n",
    "    # For optimizing the task embedding for the current task.\n",
    "    # We only optimize the task embedding corresponding to the current task,\n",
    "    # the remaining ones stay constant.\n",
    "    emb_optimizer = optim.Adam([hnet.get_task_emb(task_id)], lr=config.lr_hyper)\n",
    "\n",
    "    # Whether the regularizer will be computed during training?\n",
    "    calc_reg = task_id > 0 and config.beta > 0\n",
    "\n",
    "    # Start training iterations\n",
    "    training_iters = 0\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        \n",
    "        for sample in dataloader:\n",
    "\n",
    "            if training_iters % 1000 == 0:\n",
    "                print(f'Training iteration: {training_iters}')\n",
    "\n",
    "            ### Train theta and task embedding.\n",
    "            theta_optimizer.zero_grad()\n",
    "            emb_optimizer.zero_grad()\n",
    "\n",
    "            X = sample['x']\n",
    "            T = sample['y']\n",
    "\n",
    "            # Generate parameters of the target network for the current task\n",
    "            weights = hnet.forward(task_id)\n",
    "\n",
    "            # Using the generated weights, compute the output of the target network\n",
    "            Y = tnet.forward(X, weights)\n",
    "\n",
    "            # Add a batch dimension to the targets if it is not there\n",
    "            if len(list(T.shape))==1:\n",
    "                T = T.unsqueeze(-1)\n",
    "                \n",
    "            assert Y.shape == T.shape\n",
    "            \n",
    "            # Task-specific loss for the current task\n",
    "            loss_task = F.mse_loss(Y, T)\n",
    "\n",
    "            # Calling loss_task.backward computes the gradients w.r.t. the loss for the \n",
    "            # current task. \n",
    "            # Here we keep dtheta fixed, hence we do not need to create a graph of the derivatives\n",
    "            # and so create_graph=False\n",
    "            # The graph needs to be preserved only when the regulation loss is to be backpropagated\n",
    "            # and so retain_graph is True only when calc_reg is True\n",
    "            loss_task.backward(retain_graph=calc_reg, create_graph=False)\n",
    "\n",
    "            # The task embedding is only trained on the task-specific loss.\n",
    "            # Note, the gradients accumulated so far are from \"loss_task\".\n",
    "            emb_optimizer.step()\n",
    "\n",
    "            # Initialize the regularization loss\n",
    "            loss_reg = 0\n",
    "\n",
    "            # Initialize dTheta, the candidate change in the hnet parameters\n",
    "            dTheta = None\n",
    "\n",
    "            if calc_reg:\n",
    "\n",
    "                # Find out the candidate change (dTheta) in trainable parameters (theta) of the hnet\n",
    "                # This function just computes the change (dTheta), but does not apply it\n",
    "                dTheta = calc_delta_theta(theta_optimizer,\n",
    "                                          config.use_sgd_change, \n",
    "                                          lr=config.lr_hyper,\n",
    "                                          detach_dt=True)\n",
    "\n",
    "                # Calculate the regularization loss using dTheta\n",
    "                # This implements the second part of equation 2\n",
    "                loss_reg = calc_fix_target_reg(hnet, \n",
    "                                               task_id,\n",
    "                                               targets=targets, \n",
    "                                               dTheta=dTheta)\n",
    "\n",
    "                # Multiply the regularization loss with the scaling factor\n",
    "                loss_reg *= config.beta\n",
    "\n",
    "                # Backpropagate the regularization loss\n",
    "                loss_reg.backward()\n",
    "\n",
    "            # Update the hnet params using the current task loss and the regularization loss\n",
    "            theta_optimizer.step()\n",
    "            \n",
    "            training_iters += 1\n",
    "\n",
    "\n",
    "    print('Training network ... Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eleven-eagle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(task_id, dataloaders, tnet, hnet, device, config, ax):\n",
    "    \"\"\"\n",
    "    Evaluate the network. Evaluate the performance of the network on a\n",
    "    single task on the validation set.\n",
    "    Note, if no validation set is available, the test set will be used instead.\n",
    "    Args:\n",
    "        (....): See docstring of method :func:`train_reg`. Note, `hnet` can be\n",
    "            passed as None. In this case, no weights are passed to the `forward`\n",
    "            method of the target network.\n",
    "        train_iter: The current training iteration.\n",
    "    \"\"\"\n",
    "    print(f'Running validation after training on task_id {task_id}')\n",
    "    \n",
    "    tnet.eval()\n",
    "    if hnet is not None:\n",
    "        hnet.eval()\n",
    "        \n",
    "    colors = ['#1B9E77', '#D35C02', '#7570B3']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for val_task_id in range(config.num_tasks):\n",
    "            \n",
    "            if val_task_id > task_id:\n",
    "                continue\n",
    "            \n",
    "            dataloader = dataloaders[val_task_id]['val']\n",
    "        \n",
    "            x, y, y_true, pred = list(), list(), list(), list()\n",
    "            mse = 0.0\n",
    "            count = 0\n",
    "            for sample in dataloader:\n",
    "                \n",
    "                count += 1\n",
    "\n",
    "                x.extend(list(sample['x'].view(-1).numpy()))\n",
    "                y_true.extend(list(sample['y_true'].view(-1).numpy()))\n",
    "                y.extend(list(sample['y'].view(-1).numpy()))\n",
    "\n",
    "                X = sample['x']\n",
    "                T = sample['y']\n",
    "\n",
    "                if hnet is None:\n",
    "                    Y = tnet.forward(X)\n",
    "                else:\n",
    "                    weights = hnet.forward(val_task_id)\n",
    "                    Y = tnet.forward(X, weights)\n",
    "\n",
    "                pred.extend(list(Y.squeeze(-1).view(-1).numpy()))\n",
    "\n",
    "                if len(list(T.shape))==1:\n",
    "                    T = T.unsqueeze(-1)\n",
    "\n",
    "                assert Y.shape == T.shape\n",
    "                mse += F.mse_loss(Y, T)\n",
    "\n",
    "            x = np.array(x)\n",
    "            sort_idx = np.argsort(x)\n",
    "\n",
    "            y_true = np.array(y_true)\n",
    "            y = np.array(y)\n",
    "            pred = np.array(pred)\n",
    "\n",
    "            print(x.shape,y.shape,y_true.shape,pred.shape)\n",
    "            mse /= len(dataloader)\n",
    "\n",
    "            ax[task_id].plot(x[sort_idx], y_true[sort_idx], color=colors[task_id])\n",
    "            ax[task_id].scatter(x, pred, color=colors[task_id])\n",
    "            ax[task_id].set_title(f'MSE: {mse}')\n",
    "        \n",
    "        print('Eval - MSE loss: %f.' % (mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "unlikely-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config):\n",
    "    \n",
    "    # Check if cuda is available\n",
    "    cuda_available, device = check_cuda()\n",
    "    \n",
    "    # Set the seed for reproducability\n",
    "    set_seed(config.seed)\n",
    "        \n",
    "    # Create tasks\n",
    "    dataloaders = create_dataloaders(num_tasks=config.num_tasks,\n",
    "                                     datalen_train=config.datalen_train,\n",
    "                                     datalen_val=config.datalen_val,\n",
    "                                     batch_size=config.batch_size,\n",
    "                                     seed=config.seed)\n",
    "    \n",
    "    # Create networks\n",
    "    target_shapes = TargetNetwork.weight_shapes(n_in=config.tnet_n_in, \n",
    "                                                n_out=config.tnet_n_out, \n",
    "                                                hidden_layers=str_to_ints(config.tnet_arch), \n",
    "                                                use_bias=True)\n",
    "    hnet = HyperNetwork(layers=str_to_ints(config.hnet_arch), \n",
    "                        te_dim=config.te_dim, \n",
    "                        target_shapes=target_shapes)\n",
    "        \n",
    "    tnet = TargetNetwork(n_in=config.tnet_n_in, \n",
    "                         n_out=config.tnet_n_out, \n",
    "                         hidden_layers=str_to_ints(config.tnet_arch),\n",
    "                         activation_fn=str_to_act(config.tnet_act), \n",
    "                         use_bias=True, \n",
    "                         no_weights=True,\n",
    "                         init_weights=None, \n",
    "                         dropout_rate=-1, \n",
    "                         use_batch_norm=False, \n",
    "                         bn_track_stats=False,\n",
    "                         distill_bn_stats=False, \n",
    "                         out_fn=None)\n",
    "    \n",
    "    fig, ax = plt.subplots(config.num_tasks,1)\n",
    "    # Train sequentially on tasks\n",
    "    for task_id in range(config.num_tasks):\n",
    "                \n",
    "        # Train on the current task\n",
    "        train_reg(task_id, \n",
    "                  dataloaders[task_id]['train'], \n",
    "                  tnet, \n",
    "                  hnet, \n",
    "                  device, \n",
    "                  config)\n",
    "        \n",
    "\n",
    "        # Evaluate on the current task\n",
    "        evaluate(task_id, \n",
    "                 dataloaders, \n",
    "                 tnet, \n",
    "                 hnet, \n",
    "                 device, \n",
    "                 config,\n",
    "                 ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "formed-heart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network ...\n",
      "Training iteration: 0\n",
      "Training iteration: 1000\n",
      "Training iteration: 2000\n",
      "Training iteration: 3000\n",
      "Training network ... Done\n",
      "Running validation after training on task_id 0\n",
      "(100,) (100,) (100,) (100,)\n",
      "Eval - MSE loss: 0.006697.\n",
      "Training network ...\n",
      "Training iteration: 0\n",
      "Training iteration: 1000\n",
      "Training iteration: 2000\n",
      "Training iteration: 3000\n",
      "Training network ... Done\n",
      "Running validation after training on task_id 1\n",
      "(100,) (100,) (100,) (100,)\n",
      "(100,) (100,) (100,) (100,)\n",
      "Eval - MSE loss: 0.002867.\n",
      "Training network ...\n",
      "Training iteration: 0\n",
      "Training iteration: 1000\n",
      "Training iteration: 2000\n",
      "Training iteration: 3000\n",
      "Training network ... Done\n",
      "Running validation after training on task_id 2\n",
      "(100,) (100,) (100,) (100,)\n",
      "(100,) (100,) (100,) (100,)\n",
      "(100,) (100,) (100,) (100,)\n",
      "Eval - MSE loss: 0.007807.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = Dict2Class({'beta':0.005,\n",
    "                     'num_epochs':1000,\n",
    "                     'lr_hyper':1e-2,\n",
    "                     'num_tasks':3,\n",
    "                     'datalen_train':100,\n",
    "                     'datalen_val':100,\n",
    "                     'batch_size':32,\n",
    "                     'seed':42,\n",
    "                     'hnet_arch':'10,10',\n",
    "                     'tnet_arch':'10,10',\n",
    "                     'tnet_n_in':1,\n",
    "                     'tnet_n_out':1,\n",
    "                     'hnet_act':'sigmoid',\n",
    "                     'tnet_act':'sigmoid',\n",
    "                     'te_dim':2,\n",
    "                     'use_sgd_change': False})\n",
    "\n",
    "run(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-wilderness",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-rl",
   "language": "python",
   "name": "venv-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
