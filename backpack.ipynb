{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "passing-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GraspModel(nn.Module):\n",
    "    \"\"\"\n",
    "    An abstract model for grasp network in a common format.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GraspModel, self).__init__()\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute_loss(self, xc, yc):\n",
    "        y_pos, y_cos, y_sin, y_width = yc\n",
    "        pos_pred, cos_pred, sin_pred, width_pred = self(xc)\n",
    "\n",
    "        p_loss = F.smooth_l1_loss(pos_pred, y_pos)\n",
    "        cos_loss = F.smooth_l1_loss(cos_pred, y_cos)\n",
    "        sin_loss = F.smooth_l1_loss(sin_pred, y_sin)\n",
    "        width_loss = F.smooth_l1_loss(width_pred, y_width)\n",
    "\n",
    "        return {\n",
    "            'loss': p_loss + cos_loss + sin_loss + width_loss,\n",
    "            'losses': {\n",
    "                'p_loss': p_loss,\n",
    "                'cos_loss': cos_loss,\n",
    "                'sin_loss': sin_loss,\n",
    "                'width_loss': width_loss\n",
    "            },\n",
    "            'pred': {\n",
    "                'pos': pos_pred,\n",
    "                'cos': cos_pred,\n",
    "                'sin': sin_pred,\n",
    "                'width': width_pred\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def predict(self, xc):\n",
    "        pos_pred, cos_pred, sin_pred, width_pred = self(xc)\n",
    "        return {\n",
    "            'pos': pos_pred,\n",
    "            'cos': cos_pred,\n",
    "            'sin': sin_pred,\n",
    "            'width': width_pred\n",
    "        }\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block with dropout option\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dropout=False, prob=0.0):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.dropout1 = nn.Dropout(p=prob)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        x = self.bn1(self.conv1(x_in))\n",
    "        x = F.relu(x)\n",
    "        if self.dropout:\n",
    "            x = self.dropout1(x)\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        return x + x_in\n",
    "\n",
    "    \n",
    "class GenerativeResnet(GraspModel):\n",
    "\n",
    "    def __init__(self, input_channels=4, output_channels=1, channel_size=32, dropout=False, prob=0.0):\n",
    "        super(GenerativeResnet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, channel_size, kernel_size=9, stride=1, padding=4)\n",
    "        self.bn1 = nn.BatchNorm2d(channel_size)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(channel_size, channel_size * 2, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channel_size * 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(channel_size * 2, channel_size * 4, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(channel_size * 4)\n",
    "\n",
    "        self.res1 = ResidualBlock(channel_size * 4, channel_size * 4)\n",
    "        self.res2 = ResidualBlock(channel_size * 4, channel_size * 4)\n",
    "        self.res3 = ResidualBlock(channel_size * 4, channel_size * 4)\n",
    "        self.res4 = ResidualBlock(channel_size * 4, channel_size * 4)\n",
    "        self.res5 = ResidualBlock(channel_size * 4, channel_size * 4)\n",
    "\n",
    "        self.conv4 = nn.ConvTranspose2d(channel_size * 4, channel_size * 2, kernel_size=4, stride=2, padding=1,\n",
    "                                        output_padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(channel_size * 2)\n",
    "\n",
    "        self.conv5 = nn.ConvTranspose2d(channel_size * 2, channel_size, kernel_size=4, stride=2, padding=2,\n",
    "                                        output_padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(channel_size)\n",
    "\n",
    "        self.conv6 = nn.ConvTranspose2d(channel_size, channel_size, kernel_size=9, stride=1, padding=4)\n",
    "\n",
    "        self.pos_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)\n",
    "        self.cos_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)\n",
    "        self.sin_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)\n",
    "        self.width_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.dropout_pos = nn.Dropout(p=prob)\n",
    "        self.dropout_cos = nn.Dropout(p=prob)\n",
    "        self.dropout_sin = nn.Dropout(p=prob)\n",
    "        self.dropout_wid = nn.Dropout(p=prob)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        x = F.relu(self.bn1(self.conv1(x_in)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.res4(x)\n",
    "        x = self.res5(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        if self.dropout:\n",
    "            pos_output = self.pos_output(self.dropout_pos(x))\n",
    "            cos_output = self.cos_output(self.dropout_cos(x))\n",
    "            sin_output = self.sin_output(self.dropout_sin(x))\n",
    "            width_output = self.width_output(self.dropout_wid(x))\n",
    "        else:\n",
    "            pos_output = self.pos_output(x)\n",
    "            cos_output = self.cos_output(x)\n",
    "            sin_output = self.sin_output(x)\n",
    "            width_output = self.width_output(x)\n",
    "\n",
    "        return pos_output, cos_output, sin_output, width_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fundamental-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeResnet(input_channels=3,\n",
    "                         dropout=True,\n",
    "                         prob=0.3,\n",
    "                         channel_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adjustable-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backpack import backpack, extend\n",
    "from backpack.extensions import BatchGrad\n",
    "from backpack.utils.examples import load_one_batch_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bottom-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = extend(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-oriental",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayantan/venv/venv1/lib/python3.8/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/home/sayantan/venv/venv1/lib/python3.8/site-packages/torch/nn/modules/module.py:785: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when outputs are generated by different autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "pseudo_input = torch.rand(16,3,224,224)\n",
    "pseudo_output_pos = torch.rand(16,1,224,224)\n",
    "pseudo_output_sin = torch.rand(16,1,224,224)\n",
    "pseudo_output_cos = torch.rand(16,1,224,224)\n",
    "pseudo_output_width = torch.rand(16,1,224,224)\n",
    "pseudo_output = (pseudo_output_pos, pseudo_output_sin, pseudo_output_cos, pseudo_output_width)\n",
    "\n",
    "model.zero_grad()\n",
    "loss = model.compute_loss(pseudo_input, pseudo_output)['loss']\n",
    "\n",
    "with backpack(BatchGrad()):\n",
    "    loss.backward()\n",
    "\n",
    "print(\"{:<20}  {:<30} {:<30}\".format(\"Param\", \"grad\", \"grad (batch)\"))\n",
    "print(\"-\" * 80)\n",
    "for name, p in model.named_parameters():\n",
    "    print(\n",
    "        \"{:<20}: {:<30} {:<30}\".format(name, str(p.grad.shape), str(p.grad_batch.shape))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-editor",
   "metadata": {},
   "outputs": [],
   "source": [
    "for datapoint_idx in range(16):\n",
    "    ip = pseudo_input[datapoint_idx].unsqueeze(0)\n",
    "    op = [o[datapoint_idx].unsqueeze(0) for o i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "venv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
